{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I import all the libraries I'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.functional import F\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I set random seeds for all sources of randomness in this notebook, so that the results will be the same each time it is run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f48e80b2dd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I downloaded the book I'll be using as a data source in the notebook `1-Download`, I can simply load the text file into this notebook from the current directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reason.txt') as f:\n",
    "    fulltext = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't want the text before or after the book's actual contents, so I find the start and end points and trim the document string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7902"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = fulltext.index('PREFACE TO THE FIRST EDITION 1781')\n",
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1270832"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end = fulltext.index('End of the Project Gutenberg EBook')\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltext = fulltext[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I make the whole text lowercase so that capitalization doesn't play a role in tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltext = fulltext.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I manually tokenize the text with a few simple rules:\n",
    "- Remove all underscores, since they don't contribute any semantic meaning\n",
    "- Remove all numbers, since I only want words for my text generator\n",
    "- Only include full words (possibly with an apostrophe) and certain common punctuation that is useful for sentence structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub('_', '', text) # remove all underscores\n",
    "    text = re.sub('\\d+', '', text) # remove all numbers\n",
    "    fulltoks = re.findall(r\"[\\w']+|[,.?;:]\", text) # only include full words and wanted punctuation\n",
    "    return fulltoks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltoks = tokenize(fulltext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create a vocabulary using the Counter class from Python's standard library. It lets me rank the tokens in the order of the most commonly occurring tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [tok for tok,count in Counter(fulltoks).most_common()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to only use the top 3000 most common tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vocab[:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These classes are defined in `helpers.py` and are loosely based on the concept of transforms from the [fastai library](https://github.com/fastai/fastai). I'm using them to store the token vocabulary and for numericalization (converting lists of tokens to tensors of numbers, and vice versa):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import BaseTransform, TokTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_tfm = TokTransform(vocab)\n",
    "data = tok_tfm.encode(fulltoks) # numericalize the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I'm training a language model, I want the model to predict the following token of each input token. This means I want the input data to include everything but the last token (since the last token has no next token) and the output data to include everything but the first token (since the first token does not follow anything):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[:-1] # the inputs include everything but the last token\n",
    "y = data[1:] # the outputs include everything but the first token\n",
    "assert len(x) == len(y)\n",
    "assert len(data) == len(x)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to break the full dataset into chunks of 50 tokens, instead of one long tensor. The I will have a long list of tensors that are 50 tokens long. These will be the \"items\" that I use as inputs and group together into batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_sz = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4694"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_chunks = math.floor(len(x) / chunk_sz)\n",
    "n_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[:n_chunks*chunk_sz]; x = x.chunk(n_chunks)\n",
    "y = y[:n_chunks*chunk_sz]; y = y.chunk(n_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(x) == len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to shuffle all the chunks of text before splitting them into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_same(x_set, y_set):\n",
    "        \"Shuffle both x_set and y_set, but keep them lined up with each other\"\n",
    "        zipped = list(zip(x_set, y_set))\n",
    "        random.shuffle(zipped)\n",
    "        return list(zip(*zipped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = shuffle_same(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I'll simply take the first 80% of the pairs for the training set, and the last 20% for the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4694"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3755"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut = int(len(x) * .8)\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(range(0, 3755), range(3755, 4694))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(cut), range(cut,len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val = x[:cut], x[cut:]\n",
    "y_train, y_val = y[:cut], y[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(x_train) == len(y_train)\n",
    "assert len(x_val) == len(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These classes are again based on the concepts from `fastai`. They hold the datasets plus a batch generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import DataLoader, DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "dl_train = DataLoader(x_train, y_train, bs)\n",
    "dl_val = DataLoader(x_val, y_val, bs)\n",
    "dls = DataLoaders(dl_train, dl_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My custom model is created in `model.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import LangModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_sz = tok_tfm.count\n",
    "emb_sz = 200\n",
    "hid_sz = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LangModel(voc_sz, emb_sz, hid_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My loss function is simple cross-entropy, but first I need to flatten the predictions and the targets so that the PyTorch cross-entropy function will accept them. I also don't want to include unknown ('xxunk') tokens in the loss calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LM_loss(preds, targ):\n",
    "    preds = preds.view(-1, tok_tfm.count)\n",
    "    targ = targ.view(-1)\n",
    "    # don't include xxunk indices\n",
    "    preds = preds[targ!=0]\n",
    "    targ = targ[targ!=0]\n",
    "    return F.cross_entropy(preds, targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm simply using the Adam optimizer from PyTorch, with a learning rate that seemed to work well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics (accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using simple accuracy for my metric, but again not including unknown tokens since I only care about how many in-vocabulary tokens it predicts correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, targ):\n",
    "    # don't include xxunk indices\n",
    "    preds = preds[targ!=0]\n",
    "    targ = targ[targ!=0]\n",
    "    nums = preds.argmax(dim=-1)\n",
    "    return (nums == targ).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learner class is again based on the concepts from `fastai` but much simpler. It just groups all the objects together that I need for the training process, along with training methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, LM_loss, opt, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some experimentation, I realized my accuracy usually goes down after 3 epochs, so that's how long I'll train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 235/235\n"
     ]
    }
   ],
   "source": [
    "learn.fit(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use my logging method to see the losses and metric after each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train loss  Val loss    Metric\n",
      "0    5.291291  4.747541  0.218831\n",
      "1    4.480110  4.473159  0.236761\n",
      "2    4.182501  4.345974  0.244180\n"
     ]
    }
   ],
   "source": [
    "learn.print_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I save the trained model and the `TokTransform` object (which I will need for text generation) so that I can use them in the next notebook, `3-Generation`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘saves’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir saves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learn.model, 'saves/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tok_tfm, open('saves/tok_tfm.p', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
